{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10300621,"sourceType":"datasetVersion","datasetId":6375585},{"sourceId":10308320,"sourceType":"datasetVersion","datasetId":6381094}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers\n!pip install Unidecode\n!pip install --upgrade tensorflow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:41:29.914674Z","iopub.execute_input":"2025-01-21T09:41:29.914911Z","iopub.status.idle":"2025-01-21T09:42:51.271078Z","shell.execute_reply.started":"2025-01-21T09:41:29.914889Z","shell.execute_reply":"2025-01-21T09:42:51.270013Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nCollecting transformers\n  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nDownloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\nSuccessfully installed tokenizers-0.21.0 transformers-4.48.1\nCollecting Unidecode\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: Unidecode\nSuccessfully installed Unidecode-1.3.8\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\nCollecting tensorflow\n  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\nCollecting tensorboard<2.19,>=2.18 (from tensorflow)\n  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting keras>=3.5.0 (from tensorflow)\n  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.8.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.12.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.4)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.17.0\n    Uninstalling tensorboard-2.17.0:\n      Successfully uninstalled tensorboard-2.17.0\n  Attempting uninstall: keras\n    Found existing installation: keras 3.4.1\n    Uninstalling keras-3.4.1:\n      Successfully uninstalled keras-3.4.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.17.0\n    Uninstalling tensorflow-2.17.0:\n      Successfully uninstalled tensorflow-2.17.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.18.0 which is incompatible.\ntensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.18.0 which is incompatible.\ntf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-3.8.0 tensorboard-2.18.0 tensorflow-2.18.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install streamlit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:42:51.272247Z","iopub.execute_input":"2025-01-21T09:42:51.272567Z","iopub.status.idle":"2025-01-21T09:42:59.569209Z","shell.execute_reply.started":"2025-01-21T09:42:51.272540Z","shell.execute_reply":"2025-01-21T09:42:59.568100Z"}},"outputs":[{"name":"stdout","text":"Collecting streamlit\n  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\nRequirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\nRequirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\nRequirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\nRequirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (18.1.0)\nRequirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\nRequirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.8.1)\nRequirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\nRequirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\nRequirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\nRequirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\nDownloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydeck, streamlit\nSuccessfully installed pydeck-0.9.1 streamlit-1.41.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:42:59.570294Z","iopub.execute_input":"2025-01-21T09:42:59.570621Z","iopub.status.idle":"2025-01-21T09:43:02.573671Z","shell.execute_reply.started":"2025-01-21T09:42:59.570589Z","shell.execute_reply":"2025-01-21T09:43:02.572761Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%writefile my_app.py\nimport re\nimport string\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom datasets import Dataset\nimport torch\nfrom sklearn.metrics import f1_score, log_loss\nimport streamlit as st\n\n# --- Ánh xạ các từ viết tắt ---\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n                       \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\",\n                       \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n                       \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\",\n                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n                       \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n                       \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                       \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n                       \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n                       \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n                       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n                       \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n                       \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                       \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\",\n                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\",\n                       \"you've\": \"you have\", 'u.s': 'america', 'e.g': 'for example'}\n\n# --- Từ điển sửa lỗi chính tả ---\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do',\n                'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do',\n                'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n                'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum',\n                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota',\n                'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp',\n                'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n\n# --- Hàm clean contractions ---\ndef clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    for word in mapping.keys():\n        if \"\" + word + \"\" in text:\n            text = text.replace(\"\" + word + \"\", \"\" + mapping[word] + \"\")\n    # Remove Punctuations\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n    text = re.sub(r'[\" \"]+', \" \", text)\n    return text\n\n# --- Hàm correct spelling ---\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x\n\n# --- Hàm tiền xử lý chính ---\ndef preprocess_data(text):\n    text = clean_contractions(text, contraction_mapping)\n    text = correct_spelling(text, mispell_dict)\n    return text\n\n# --- Load Models and Tokenizers ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel_ids = {\n    \"xlnet\": \"sercetexam9/cs221-xlnet-large-cased-eng-finetuned-20-epochs-tapt\",\n    \"roberta\": \"Kuongan/CS221-roberta-base-finetuned-semeval-new\",\n    \"deberta\": \"sercetexam9/cs221-deberta-base-multi-label-classifier-eng-finetuned-30-epochs-tapt\"\n}\n\nmodels = {}\ntokenizers = {}\n\nfor model_name, model_id in model_ids.items():\n    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=5)\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model.to(device)\n    model.eval()\n    models[model_name] = model\n    tokenizers[model_name] = tokenizer\nprint('SUCCESS!')\n# --- Prediction Function (Soft Voting) ---\nmax_len = 64\nemotions = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n\ndef predict_emotions_soft_voting(text, threshold=0.3):\n    start =time.time()\n    predictions = {emotion: [] for emotion in emotions}\n\n    # DeBERTa prediction\n    text_deberta = preprocess_data(text)\n    tokenizer_deberta = tokenizers[\"deberta\"]\n    inputs_deberta = tokenizer_deberta(\n        text_deberta,\n        padding='max_length',\n        truncation=True,\n        max_length=max_len,\n        return_tensors='pt'\n    )\n    input_ids_deberta = inputs_deberta['input_ids'].to(device)\n    attention_mask_deberta = inputs_deberta['attention_mask'].to(device)\n    with torch.no_grad():\n        outputs_deberta = models[\"deberta\"](input_ids=input_ids_deberta, attention_mask=attention_mask_deberta)\n        logits_deberta = outputs_deberta.logits\n        probs_deberta = torch.sigmoid(logits_deberta).cpu().numpy()\n\n    # RoBERTa and XLNet predictions\n    text_roberta_xlnet = preprocess_data(text)\n    for model_name in [\"roberta\", \"xlnet\"]:\n        tokenizer = tokenizers[model_name]\n        inputs = tokenizer(\n            text_roberta_xlnet,\n            padding='max_length',\n            truncation=True,\n            max_length=max_len,\n            return_tensors='pt'\n        )\n        input_ids = inputs['input_ids'].to(device)\n        attention_mask = inputs['attention_mask'].to(device)\n        if 'token_type_ids' in inputs:\n            token_type_ids = inputs['token_type_ids'].to(device)\n            model_inputs = {'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}\n        else:\n            model_inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n\n        with torch.no_grad():\n            outputs = models[model_name](**model_inputs)\n            logits = outputs.logits\n            probs = torch.sigmoid(logits).cpu().numpy()\n\n            for i, emotion in enumerate(emotions):\n                if model_name == \"roberta\":\n                    predictions[emotion].append(probs[0][i])\n                else: #xlnet\n                    predictions[emotion].append(probs[0][i])\n\n    # Combine predictions\n    final_probs = []\n    for i, emotion in enumerate(emotions):\n        if emotion == 'anger':\n            final_probs.append((probs_deberta[0][i] * 2 + predictions[emotion][0] + predictions[emotion][1])/4) # DeBERTa * 2\n        elif emotion == 'surprise':\n            final_probs.append((predictions[emotion][1] * 2 + predictions[emotion][0] + probs_deberta[0][i])/4) # XLNet * 2\n        else:\n            final_probs.append((probs_deberta[0][i] + predictions[emotion][0] + predictions[emotion][1])/3)\n\n    binary_preds = (np.array(final_probs) >= threshold).astype(int)\n    return binary_preds.flatten()\n\n# --- Streamlit App ---\nst.set_page_config(page_title=\"Sentiment Analysis\", page_icon=\"🔍\", layout=\"wide\")\nst.markdown(\"\"\"\n    <style>\n    .main {\n        background-color: #f0f2f6;\n        padding: 2rem;\n    }\n    .stButton>button {\n        color: white;\n        background-color: #4CAF50;\n        border: none;\n        padding: 12px 28px;\n        text-align: center;\n        text-decoration: none;\n        display: inline-block;\n        font-size: 16px;\n        margin: 4px 2px;\n        cursor: pointer;\n        border-radius: 4px;\n    }\n    .stTextInput>div>div>input {\n        padding: 10px;\n        font-size: 16px;\n        border-radius: 4px;\n        border: 1px solid #ccc;\n    }\n    .stSelectbox>div>div>div {\n        padding: 10px;\n        font-size: 16px;\n        border-radius: 4px;\n        border: 1px solid #ccc;\n    }\n    .stTextArea textarea {\n        border: 2px solid #4CAF50;\n        border-radius: 5px;\n        padding: 10px;\n        font-size: 16px;\n    }\n    h1 {\n        text-align: center;\n        color: #FF8C00;\n        font-family: 'Georgia', serif;\n    }\n    </style>\n    <h1> 🔍 NTA  </h1>\n\"\"\", unsafe_allow_html=True)\n\nst.title(\"🔍 Sentiment Analysis Interface\")\nst.markdown(\"\"\"\n    Welcome to the Sentiment Analysis Interface. Use the form below to enter your sentence and we will predict the emotion of the context.\n    \"\"\")\n\ntext_input = st.text_area(\"Nhập văn bản vào đây:\")\n\nif st.button(\"Predict\", type=\"primary\"):\n    start=time.time()\n    predictions = predict_emotions_soft_voting(text_input)\n    predicted_emotions = [emotions[i] for i, pred in enumerate(predictions) if pred == 1]\n    end=time.time()-start\n    if predicted_emotions:\n        emotion_str = \", \".join(predicted_emotions)\n        st.write(f\"Cảm xúc dự đoán: {emotion_str}.\")\n    else:\n        st.write(\"Cảm xúc dự đoán: không thuộc các cảm xúc: anger, fear, joy, sadness hay surprise.\")\n    st.write(f\"Tổng thời gian xử lý: {end}s.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:47:26.923019Z","iopub.execute_input":"2025-01-21T09:47:26.923334Z","iopub.status.idle":"2025-01-21T09:47:26.931540Z","shell.execute_reply.started":"2025-01-21T09:47:26.923307Z","shell.execute_reply":"2025-01-21T09:47:26.930586Z"}},"outputs":[{"name":"stdout","text":"Overwriting my_app.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!curl https://loca.lt/mytunnelpassword","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:43:02.585138Z","iopub.execute_input":"2025-01-21T09:43:02.585405Z","iopub.status.idle":"2025-01-21T09:43:03.390690Z","shell.execute_reply.started":"2025-01-21T09:43:02.585385Z","shell.execute_reply":"2025-01-21T09:43:03.389661Z"}},"outputs":[{"name":"stdout","text":"104.196.239.167","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!streamlit run my_app.py & yes | npx localtunnel --port 8501","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T09:47:28.982427Z","iopub.execute_input":"2025-01-21T09:47:28.982786Z","execution_failed":"2025-01-21T09:52:12.210Z"}},"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.196.239.167:8501\u001b[0m\n\u001b[0m\nyour url is: https://plain-files-laugh.loca.lt\n2025-01-21 09:47:46.497210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737452866.523848     200 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737452866.531973     200 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSUCCESS!\nSUCCESS!\nSUCCESS!\n","output_type":"stream"}],"execution_count":null}]}